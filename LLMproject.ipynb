#the last code
# Step 1: Mount Google Drive 'we used google colab'
from google.colab import drive
drive.mount('/content/drive')

# Install required libraries
!pip install openai==0.28
!pip install beautifulsoup4
!pip install requests

# Import needed libraries
import pandas as pd
import json
import openai
import os
import requests
from bs4 import BeautifulSoup
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Step 2: Load "TAWASUL" dataset
training_data_new = pd.read_csv('/content/drive/MyDrive/training_data_new.csv')

# Step 3: Convert dataset to GPT-3.5 fine-tuning format with augmentation
def convert_to_gpt35_format_with_augmentation(training_data_new):
    fine_tuning_data = []
    for _, row in training_data_new.iterrows():
        user_messages = []

        # Add more than one form of the question
        if not pd.isna(row['الاستفسارات الظاهرة للمستفيد الخارجي']):
            user_messages.append({"role": "user", "content": row['الاستفسارات الظاهرة للمستفيد الخارجي']})
        if 'الاستفسار بصياغة أخرى - 1' in training_data_new.columns and not pd.isna(row['الاستفسار بصياغة أخرى - 1']):
            user_messages.append({"role": "user", "content": row['الاستفسار بصياغة أخرى - 1']})
        if 'الاستفسار بصياغة أخرى - 2' in training_data_new.columns and not pd.isna(row['الاستفسار بصياغة أخرى - 2']):
            user_messages.append({"role": "user", "content": row['الاستفسار بصياغة أخرى - 2']})

        # Add assistant's response
        fine_tuning_data.append({
            "messages": user_messages + [
                {"role": "assistant", "content": row['الإجابة']}  # Assistant's response
            ]
        })
    return fine_tuning_data

# Step 4: Write fine-tuning data to JSONL format
def write_to_jsonl(data, output_file):
    with open(output_file, 'w') as file:
        for entry in data:
            json.dump(entry, file)
            file.write('\n')

# Step 5: Set OpenAI API key
openai.api_key = "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"  

# Step 6:  fine-tuned model id
fine_tuned_model_id = "ft:gpt-3.5-turbo-0125:personal:tawasul:AJTrAMFX"  

# Step 7: Function to get response from OpenAI fine-tuned model
def get_response_from_openai(user_query):
    response = openai.ChatCompletion.create(
        model=fine_tuned_model_id,
        messages=[
            {"role": "user", "content": user_query}
        ],
         temperature=0.1  # Lower the temperature for more focused answers
    )
    return response['choices'][0]['message']['content']

# Step 8: embedding-based retrieval
def generate_question_embeddings(data):
    questions = data['الاستفسارات الظاهرة للمستفيد الخارجي'].tolist()

    # Batch embedding generation
    response = openai.Embedding.create(input=questions, model="text-embedding-ada-002")
    embeddings = [result['embedding'] for result in response['data']]

    # Map questions to their embeddings
    question_embeddings = {question: embedding for question, embedding in zip(questions, embeddings)}

    return question_embeddings

# Generate embeddings 
question_embeddings = generate_question_embeddings(training_data_new)

# Step 9: Get the closest answer based on embeddings with thresholding
def get_closest_answer(user_query, question_embeddings, data, threshold=0.7):
    # Generate embedding for user query
    user_query_embedding = openai.Embedding.create(input=[user_query], model="text-embedding-ada-002")['data'][0]['embedding']

    # Calculate cosine similarities
    similarities = {}
    for question, embedding in question_embeddings.items():
        similarity = cosine_similarity([user_query_embedding], [embedding])[0][0]
        similarities[question] = similarity

    # Get best match and check if it passes the threshold
    best_match = max(similarities, key=similarities.get)
    best_similarity = similarities[best_match]

    if best_similarity < threshold:
        return None  # Return None if no match passes the threshold

    # Return the corresponding answer
    answer = data.loc[data['الاستفسارات الظاهرة للمستفيد الخارجي'] == best_match]['الإجابة'].values[0]
    return answer

# Step 9: Get the closest answer based on embeddings with thresholding
def get_closest_answer(user_query, question_embeddings, data, threshold=0.7):
    # Generate embedding for user query
    user_query_embedding = openai.Embedding.create(input=[user_query], model="text-embedding-ada-002")['data'][0]['embedding']

    # Calculate cosine similarities
    similarities = {}
    for question, embedding in question_embeddings.items():
        similarity = cosine_similarity([user_query_embedding], [embedding])[0][0]
        similarities[question] = similarity

    # Get best match and check if it passes the threshold
    best_match = max(similarities, key=similarities.get)
    best_similarity = similarities[best_match]

    if best_similarity < threshold:
        return None  # Return None if no match passes the threshold

    # Return the corresponding answer
    answer = data.loc[data['الاستفسارات الظاهرة للمستفيد الخارجي'] == best_match]['الإجابة'].values[0]
    return answer

# Step 11: Combine embedding-based retrieval, fine-tuned model, and FAQs
def get_response(user_query):
    # Retrieve the closest answer based on the embedding approach
    embedding_answer = get_closest_answer(user_query, question_embeddings, training_data_new)

    if embedding_answer:
        # Use the fine-tuned model with the embedding-based answer as a context
        fine_tuned_response = openai.ChatCompletion.create(
            model="ft:gpt-3.5-turbo-0125:personal:tawasul:AJTrAMFX",
            messages=[
                {"role": "user", "content": user_query},
                {"role": "system", "content": f"The following is the best match retrieved from our FAQ database: '{embedding_answer}'."},
                {"role": "assistant", "content": "Based on the information provided, I would respond as follows:"}
            ],
            temperature=0.3  # Lower the temperature for more focused answers
        )['choices'][0]['message']['content']
    else:
        # If no embedding-based answer is found, directly use the fine-tuned model
        fine_tuned_response = get_response_from_openai(user_query)

    return f"Embedding-based Answer: {embedding_answer}\nFine-tuned Model Answer: {fine_tuned_response}"

# Step 12: Fetch FAQs
url = "https://moe.gov.sa/ar/aboutus/Portal/Pages/FAQs.aspx"
faqs = fetch_faqs_from_url(url)
for faq in faqs:
    print(f"Question: {faq['question']}\nAnswer: {faq['answer']}\n")

# Step 13: Test with a sample query
user_query = "هل يوجد تعميم بشأن تغيير موعد  وجدول اختبارات الجانب التحريري للمواد ذات الجانبين مثل (الحاسب الألي ... المواد التطبيقية) للطلاب؟"
answer = get_response(user_query)
print(f"user query: {user_query}")
print(f"Answer: {answer}")
